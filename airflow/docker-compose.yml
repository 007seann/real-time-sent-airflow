
services:
    postgres:
        image: postgres:13
        container_name: postgres_container
        environment:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflowMetadata
        volumes:
          - postgres-db-volume:/var/lib/postgresql/data
          - ./init_db_postgres.sh:/docker-entrypoint-initdb.d/init_db.sh:ro
        healthcheck:
          test: ["CMD", "pg_isready", "-U", "airflow"]
          interval: 5s
          retries: 5
        restart: always
        
    redis:
        image: redis:latest
        expose:
          - 6379
        healthcheck:
          test: ["CMD", "redis-cli", "ping"]
          interval: 5s
          timeout: 30s
          retries: 50
        restart: always    

    airflow:       
        # # privileged: true
        # user: "root"                           
        build: './airflow_docker'                       
        container_name: airflow_container
        restart: always
        depends_on:
            - postgres
        environment:
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_container:5432/airflow
            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'True'
            AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
            AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
            AIRFLOW__CORE__FERNET_KEY: 'ffHBWrWkkmkrpqEu_9fz8-sF0SdI-OAvUQ1OeQNoomE='
            AIRFLOW__CORE__PARALLELISM: 4
            AIRFLOW__CORE__DAG_CONCURRENCY: 4
            AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 120
            dags_folder: /opt/airflow/dags
            WEB_SERVER_PORT: 3000
            NLTK_DATA: /opt/airflow/nltk_data
            PYTHONPATH: /home/airflow/.local/lib/python3.7/site-packages:$PYTHONPATH:/opt/airflow/plugins
            PATH: /home/airflow/.local/bin:/usr/lib/jvm/java-11-openjdk-amd64/bin:/opt/spark-3.2.1-bin-hadoop3.2/bin:/opt/spark-3.2.1-bin-hadoop3.2/sbin:/opt/hadoop-3.3.1/bin:/opt/hive-3.1.3-bin/bin:$PATH
            YFINANCE_CACHE_DIR: /opt/airflow/.cache/py-yfinance
            SPARK_HOME: /opt/spark-3.2.1-bin-hadoop3.2
            JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
            PYSPARK_SUBMIT_ARGS: "--packages org.postgresql:postgresql:42.2.20"
        volumes:                                        
            - ./dags:/opt/airflow/dags:rw
            - ./data:/opt/airflow/data:rw
            - ./plugins:/opt/airflow/plugins:rw
            - ./nltk_data:/opt/airflow/nltk_data
            - ./spark_logs:/opt/spark-3.2.1-bin-hadoop3.2/logs:rw
            - ./airflow_docker/airflow_env.sh:/opt/airflow/airflow_env.sh:ro


        ports:
            - "8080:8080"   # Airflow Webserver
            - "3000:3000"   # Custom Webserver Port
            - "4040:4040"   # Spark UI

        command: webserver
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:3000/health"]
          interval: 10s
          timeout: 10s
          retries: 5


volumes:
  postgres-db-volume:
