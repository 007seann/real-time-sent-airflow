# Use an official Python runtime as a base image
FROM python:3.9-slim AS base

# Upgrade pip (optional, but recommended)
RUN python -m pip install --upgrade pip

# Set environment variable for Airflow home directory
ENV AIRFLOW_HOME=/opt/airflow

# Install system dependencies
USER root
RUN apt-get update && apt-get install -y \
    openjdk-11-jre wget supervisor build-essential libpq-dev curl \
    && apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get install -y wget

# Install Airflow
FROM apache/airflow:2.5.1-python3.9
USER root
RUN apt-get update && apt-get install -y wget openjdk-11-jre supervisor build-essential libpq-dev curl && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

USER airflow
RUN pip install --no-cache-dir apache-airflow==2.5.1

# Install Spark
USER root
ENV SPARK_VERSION=3.2.1
ENV SPARK_HADOOP_VERSION=3.2
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz -C /opt/ && \
    rm spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}
ENV PATH=$PATH:${SPARK_HOME}/bin:${SPARK_HOME}/sbin

# Install Java
USER root
RUN apt-get update && \
    apt-get install -y openjdk-11-jre wget supervisor && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Install Python requirements
USER airflow
WORKDIR /opt/airflow/
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Set up Airflow user
USER root
RUN if ! getent group airflow > /dev/null; then \
        addgroup --gid 1000 airflow; \
    fi && \
    if ! getent passwd airflow > /dev/null; then \
        adduser --uid 1000 --ingroup airflow --home /opt/airflow --no-create-home airflow; \
    fi

# Set permissions for Airflow directories
RUN mkdir -p /opt/airflow/logs && chown -R airflow:airflow /opt/airflow

# Copy necessary files
COPY airflow_env.sh /opt/airflow/airflow_env.sh
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Expose necessary ports
EXPOSE 8000

# Switch to airflow user
USER airflow

# Set entrypoint and default command
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]